{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    cifar10.py\n",
    "\"\"\"\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS] [--batch-size BATCH_SIZE]\n",
      "                             [--subset-train SUBSET_TRAIN]\n",
      "                             [--subset-val SUBSET_VAL] [--gpu] [--seed SEED]\n",
      "                             [--data-dir DATA_DIR]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Raymond\\AppData\\Roaming\\jupyter\\runtime\\kernel-47614d99-6c64-4682-8f5c-f17d728eafcd.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# --\n",
    "# CLI\n",
    "# The default values of most arguments are sufficient for this problem\n",
    "# You may want to use --subset-train or --subset-val to use only a subset\n",
    "# of the data while testing your code.\n",
    "# You may want to use --data-dir to save the CIFAR-10 dataset in a\n",
    "# specific location\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--epochs', type=int, default=1)\n",
    "    parser.add_argument('--batch-size', type=int, default=128)\n",
    "    parser.add_argument('--subset-train', type=int, default=None)\n",
    "    parser.add_argument('--subset-val', type=int, default=None)\n",
    "    parser.add_argument('--gpu', default=False, action=\"store_const\", const=True)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    parser.add_argument('--data-dir', default='./data')\n",
    "    return parser.parse_args()\n",
    "\n",
    "args = parse_args()\n",
    "\n",
    "# --\n",
    "# Loading data\n",
    "def load_data():\n",
    "    print('cifar10.py: making dataloaders...', file=sys.stderr) \n",
    "    # transforms define preprocessing on the dataset   \n",
    "    transform_train = transforms.Compose([\n",
    "        # TO IMPLEMENT Part (d) random crop\n",
    "        # TO IMPLEMENT Part (d) random horizontal flip\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std= (0.24705882352941178, 0.24352941176470588, 0.2615686274509804))\n",
    "    ])\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=(0.4914, 0.4822, 0.4465), std= (0.24705882352941178, 0.24352941176470588, 0.2615686274509804))\n",
    "    ])\n",
    "    # loading the dataset\n",
    "    try:\n",
    "        trainset = datasets.CIFAR10(root=args.data_dir, train=True, download=True, transform=transform_train)\n",
    "        valset  = datasets.CIFAR10(root=args.data_dir, train=False, download=True, transform=transform_val)\n",
    "    except:\n",
    "        raise Exception('cifar10.py: error loading data -- try rerunning w/ `--download` flag')\n",
    "    # selecting a random subset of the dataset if requested\n",
    "    np.random.seed(args.seed)\n",
    "    if (args.subset_train is not None):\n",
    "        idxs = np.random.choice(len(trainset), args.subset_train, replace=False)\n",
    "        trainset = torch.utils.data.Subset(trainset, idxs)\n",
    "    if (args.subset_val is not None):\n",
    "        idxs = np.random.choice(len(valset), args.subset_val, replace=False)\n",
    "        trainset = torch.utils.data.Subset(valset, idxs)\n",
    "    # data loading objects\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    valloader = torch.utils.data.DataLoader(\n",
    "        valset,\n",
    "        batch_size=512,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    dataloaders = {\n",
    "        \"train\" : trainloader,\n",
    "        \"val\"  : valloader,\n",
    "    }\n",
    "    return dataloaders\n",
    "\n",
    "dataloaders = load_data()\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# -- \n",
    "# Linear model methods\n",
    "def get_X_Y(loader):\n",
    "    X = []; Y = []\n",
    "    for data in loader:\n",
    "        for x in data[0]:\n",
    "            X.append(x.numpy())\n",
    "        for y in data[1]:\n",
    "            Y.append(y.numpy())\n",
    "    X = np.array(X); Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "def get_PCA_embedding(X, k=2):\n",
    "    pca = PCA(n_components=k)\n",
    "    pca.fit(X)\n",
    "    X_p = pca.transform(X)\n",
    "    return X_p\n",
    "\n",
    "def get_CCA_embedding(X, Y, k=2):\n",
    "    cca = CCA(n_components=k)\n",
    "    cca.fit(X, Y)\n",
    "    X_c, Y_c = cca.transform(X, Y)\n",
    "    return X_c, Y_c\n",
    "\n",
    "# --\n",
    "# Model definition\n",
    "# Derived from models in `https://github.com/kuangliu/pytorch-cifar`\n",
    "class PreActBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn1   = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2   = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(x))\n",
    "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
    "        out = self.conv1(out)\n",
    "        out = self.conv2(F.relu(self.bn2(out)))\n",
    "        return out + shortcut\n",
    "\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_blocks=[2, 2, 2, 2], num_classes=10):\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.prep = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            self._make_layer(64, 64, num_blocks[0], stride=1),\n",
    "            self._make_layer(64, 128, num_blocks[1], stride=2),\n",
    "            self._make_layer(128, 256, num_blocks[2], stride=2),\n",
    "            self._make_layer(256, 256, num_blocks[3], stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(512, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
    "\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(PreActBlock(in_channels=in_channels, out_channels=out_channels, stride=stride))\n",
    "            in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = self.prep(x)\n",
    "        x = self.layers(x)\n",
    "\n",
    "        x_avg = F.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x_avg = x_avg.view(x_avg.size(0), -1)\n",
    "\n",
    "        x_max = F.adaptive_max_pool2d(x, (1, 1))\n",
    "        x_max = x_max.view(x_max.size(0), -1)\n",
    "\n",
    "        x = torch.cat([x_avg, x_max], dim=-1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# --\n",
    "# Learning rate functions\n",
    "def set_lr(lr, optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "def const_lr_maker(const=0.001):\n",
    "    # TO IMPLEMENT Part (e)\n",
    "    return None \n",
    "    return f\n",
    "\n",
    "def anneal_lr_maker(factor=0.99):\n",
    "    # TO IMPLEMENT Part (e)\n",
    "    return factor\n",
    "\n",
    "def special_lr_maker(hp_max=0.1, epochs=3, hp_init=0.0, hp_final=0.005, extra=5):\n",
    "    def f(curr_lr, epoch, batch_idx, N):\n",
    "        progress = epoch + batch_idx / N\n",
    "        if progress < epochs / 2:\n",
    "            return 2 * hp_max * (1 - (epochs - progress) / epochs)\n",
    "        elif progress <= epochs:\n",
    "            return hp_final + 2 * (hp_max - hp_final) * (epochs - progress) / epochs\n",
    "        elif progress <= epochs + extra:\n",
    "            return hp_final * (extra - (progress - epochs)) / extra\n",
    "        else:\n",
    "            return hp_final / 10\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'const_lr_maker' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-e9f074281a10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconst_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconst_lr_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0manneal_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manneal_lr_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mspecial_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecial_lr_maker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# --\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'const_lr_maker' is not defined"
     ]
    }
   ],
   "source": [
    "const_lr = const_lr_maker()\n",
    "anneal_lr = anneal_lr_maker()\n",
    "special_lr = special_lr_maker(epochs=args.epochs+3)\n",
    "\n",
    "# --\n",
    "# Model training\n",
    "def train_with_lr_scheme(calc_lr):\n",
    "    \n",
    "    print('cifar10.py: initializing model...', file=sys.stderr)\n",
    "    if (torch.cuda.is_available() and args.gpu):\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = ResNet18().to(device)\n",
    "    model.verbose = True\n",
    "    # --\n",
    "    # Initialize optimizer\n",
    "    print('cifar10.py: training...', file=sys.stderr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    lr = 0.01\n",
    "\n",
    "    trainloader, valloader = dataloaders['train'], dataloaders['val']\n",
    "    N = len(trainloader)\n",
    "    iter_count = 0 # a count of all iterations\n",
    "    for epoch in range(args.epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        total_train = 0; correct_train = 0\n",
    "        total_val = 0; correct_val = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (torch.cuda.is_available() and args.gpu):\n",
    "                labels = labels.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "            # forward + backward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize\n",
    "            lr = calc_lr(lr, epoch, i, N)\n",
    "            set_lr(lr, optimizer)\n",
    "            iter_count += 1\n",
    "            optimizer.step()\n",
    "\n",
    "            # compute training statistics\n",
    "            logits = outputs.cpu().detach().numpy()\n",
    "            y_pred_train = np.argmax(logits, axis=1)\n",
    "            y_train = labels.cpu().detach().numpy()\n",
    "            total_train += y_train.shape[0]\n",
    "            correct_train += sum(y_pred_train == y_train)\n",
    "            running_loss += loss.item()\n",
    "            print(\"Epoch:\", epoch, \"\\tMiniBatch:\", i, \"\\tPartial Training Accuracy:\", correct_train/total_train,  \"\\tRunning Loss:\", running_loss/(i+1))\n",
    "        print(\"Epoch:\", epoch, \"\\tFinal Training Accuracy:\", {correct_train/total_train})\n",
    "        for i, data in enumerate(valloader, 0):\n",
    "            # get the inputs\n",
    "            inputs, labels = data\n",
    "            if (torch.cuda.is_available() and args.gpu):\n",
    "                labels = labels.cuda()\n",
    "                inputs = inputs.cuda()\n",
    "            # predict outputs\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs.cpu().detach().numpy()\n",
    "\n",
    "            # compute validation statistics\n",
    "            y_pred_train = np.argmax(logits, axis=1)\n",
    "            y_train = labels.cpu().detach().numpy()\n",
    "            total_val += y_train.shape[0]\n",
    "            correct_val  += sum(y_pred_train == y_train)\n",
    "            print(\"Epoch:\", epoch, \"\\tMiniBatch:\", i, \"\\tPartial Validation Accuracy:\", correct_val/total_val)\n",
    "        print(\"Epoch:\", epoch, \"\\tFinal Validation Accuracy:\", {correct_val/total_val})\n",
    "        result = {}\n",
    "        result['train_accuracy'] = correct_train/total_train\n",
    "        result['val_accuracy'] = correct_val/total_val\n",
    "        result['num_epochs'] = args.epochs\n",
    "        result['train_loss'] = running_loss \n",
    "    return result\n",
    "\n",
    "VISUALIZATION_FLAG = False # Part (a)\n",
    "LINEAR_FLAG = True # Part (b)\n",
    "PRINT_NN_FLAG = False # Part (c)\n",
    "TRAIN_NN_FLAG = False # Parts (d-f)\n",
    "\n",
    "if VISUALIZATION_FLAG:\n",
    "    print(\"===== Constructing data matrices ====\")\n",
    "    X, labels = get_X_Y(dataloaders['train'])\n",
    "    X_val, labels_val = get_X_Y(dataloaders['val'])\n",
    "\n",
    "    # vectorizing images in X\n",
    "    X = X.reshape(X.shape[0],-1)\n",
    "    X_val = X_val.reshape(X_val.shape[0],-1)\n",
    "\n",
    "    # changing Y to one-hot encoding\n",
    "    Y = np.eye(10)[labels]\n",
    "\n",
    "    # PCA embedding\n",
    "    print(\"===== Computing PCA Embedding ====\")\n",
    "    Xp = get_PCA_embedding(X)\n",
    "\n",
    "    # CCA embedding \n",
    "    print(\"===== Computing CCA Embedding ====\")\n",
    "    Xc, _ = get_CCA_embedding(X, Y)\n",
    "\n",
    "    print(Xp.shape)\n",
    "    plt.scatter(Xp[:,0],Xp[:,1])\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting visualization\n",
    "    # TO IMPLEMENT Part (a)\n",
    "    \n",
    "if LINEAR_FLAG:\n",
    "    print(\"===== Constructing data matrices ====\")\n",
    "    X, labels = get_X_Y(dataloaders['train'])\n",
    "    X_val, labels_val = get_X_Y(dataloaders['val'])\n",
    "\n",
    "    # vectorizing images in X\n",
    "    X = X.reshape(X.shape[0],-1)\n",
    "    X_val = X_val.reshape(X_val.shape[0],-1)\n",
    "\n",
    "    # changing Y to one-hot encoding\n",
    "    Y = np.eye(10)[labels]\n",
    "\n",
    "    # Simple linear classifier on raw data\n",
    "    print(\"===== Fitting linear model ====\")\n",
    "    lin = linear_model.LinearRegression().fit(X, Y)\n",
    "\n",
    "    # test accuracy\n",
    "    Y_pred = lin.predict(X) # train predictions\n",
    "    labels_pred = np.argmax(Y_pred, axis=1) # converting one-hot prediction to label\n",
    "    train_accuracy = np.sum(labels_pred == labels) / len(labels)\n",
    "\n",
    "    # validation accuracy\n",
    "    Y_pred_val = lin.predict(X_val) # validation predictions\n",
    "    labels_pred_val = np.argmax(Y_pred_val, axis=1) # converting one-hot prediction to label\n",
    "    val_accuracy = np.sum(labels_pred_val == labels_val) / len(labels_val)\n",
    "\n",
    "    print(\"===== Linear Model Results ====\")\n",
    "    print('train accuracy:\\t',train_accuracy)\n",
    "    print('validation accuracy:\\t',val_accuracy)\n",
    "if PRINT_NN_FLAG:\n",
    "    model = ResNet18().to(torch.device('cpu'))\n",
    "    model.verbose = True\n",
    "    print(model)\n",
    "if TRAIN_NN_FLAG: \n",
    "    print(\"===== Training network with a constant LR scheme ====\")\n",
    "    result_const = train_with_lr_scheme(const_lr)\n",
    "    result_const['lr_scheme'] = 'const'\n",
    "    print(\"===== Training network with an annealing LR scheme ====\")\n",
    "    result_anneal = train_with_lr_scheme(anneal_lr)\n",
    "    result_anneal['lr_scheme'] = 'anneal'\n",
    "    print(\"===== Training network with a special LR scheme ====\")\n",
    "    result_special = train_with_lr_scheme(special_lr)\n",
    "    result_special['lr_scheme'] = 'special_0'\n",
    "    print(\"==== Summary ===\")\n",
    "    df = pd.DataFrame([result_const, result_anneal, result_special])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
